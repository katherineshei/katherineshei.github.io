---
title: 'Project 2: Modeling, Testing, and Predicting'
author: "Katherine Shei; kjs3639"
date: ''
output:
  html_document:
    toc: yes
    toc_float:
      collapsed: no
      smooth_scroll: yes
  pdf_document:
    toc: yes
  word_document:
    toc: yes
---

```{r setup, include=FALSE}
library(knitr)
hook_output = knit_hooks$get('output')
knit_hooks$set(output = function(x, options) {
  # this hook is used only when the linewidth option is not NULL
  if (!is.null(n <- options$linewidth)) {
    x = knitr:::split_lines(x)
    # any lines wider than n should be wrapped
    if (any(nchar(x) > n)) x = strwrap(x, width = n)
    x = paste(x, collapse = '\n')
  }
  hook_output(x, options)
})

knitr::opts_chunk$set(echo = TRUE, eval = TRUE,fig.align="center",warning=FALSE,message=FALSE,fig.width=8, fig.height=5, linewidth=60)
options(tibble.width = 100,width = 100)
library(tidyverse)
```

#Introduction
The dataset I'm working with has descriptors of the US Atlantic Hurricanes with the main variables "Name" (hurricane name), "Year" (hurricane year), "LF.WindsMPH" (maximum sustained windspeed), "LF.PressureMB" (atmospheric pressure at landfall), "firstLF" (date of first landfall), "BaseDamage" (property damage in millions of dollars), "deaths" (number of continental US deaths), and "mf" (gender of name). These variables measure the force of the hurricane, its affect on various states and people's lives, and its year of occurance and name. This dataset has 94 observations and 12 variables.

#Dataset
```{r}
install.packages('DAAG', repos= 'https://cran.r-project.org/src/contrib/Archive/DAAG/DAAG_1.22.tar.gz', type='source')
library(DAAG)
head(hurricNamed, 3)

hurricane_name <- hurricNamed
```


#MANOVA/ANOVA
```{r}
  #Creating WindSpeed Category Categorical Variable
hurricane <- hurricane_name %>% mutate(windsMPH_cat = case_when(LF.WindsMPH>=74 & LF.WindsMPH<=95 ~ "One", LF.WindsMPH>=96 & LF.WindsMPH<=110 ~ "Two", LF.WindsMPH>=111 & LF.WindsMPH<=129 ~ "Three", LF.WindsMPH>=130 & LF.WindsMPH<=156 ~ "Four", LF.WindsMPH>=157 ~ "Five"))

  #MANOVA
man_h <- manova(cbind(LF.WindsMPH, LF.PressureMB, BaseDam2014, BaseDamage, NDAM2014, deaths) ~ windsMPH_cat, data=hurricane)
summary(man_h)

  #ANOVA
summary.aov(man_h)

  #Post-hoc T Tests
pairwise.t.test(hurricane$LF.WindsMPH,hurricane$windsMPH_cat, p.adj="none") 
pairwise.t.test(hurricane$LF.PressureMB,hurricane$windsMPH_cat, p.adj="none") 
pairwise.t.test(hurricane$NDAM2014,hurricane$windsMPH_cat, p.adj="none") 

  #Probability of at least One Type I Error: 1 MANOVA, 6 ANOVAS, 30 post-hoc t tests were performed, with a total of 37 tests
1-(.95^37)
0.05/37 #Bonferroni Correction

  #Post-hoc T Tests with Bonferroni Correction
pairwise.t.test(hurricane$LF.WindsMPH,hurricane$windsMPH_cat, p.adj="bonferroni")
pairwise.t.test(hurricane$LF.PressureMB,hurricane$windsMPH_cat, p.adj="bonferroni") 
pairwise.t.test(hurricane$NDAM2014,hurricane$windsMPH_cat, p.adj="bonferroni") 

  #Assumptions:
library(rstatix)

hurricane_assump <- hurricane %>% filter(!windsMPH_cat == "Five")
group <- hurricane_assump$windsMPH_cat
DVs <- hurricane_assump %>% select(LF.WindsMPH, LF.PressureMB, BaseDam2014, BaseDamage, NDAM2014, deaths)

#Test multivariate normality for each group (null: assumption met)
sapply(split(DVs,group), mshapiro_test)
```
A one-way MANOVA was conducted to determine the effects of the hurricane windspeed category (One, Two, Three, Four, Five) on six dependent variables (LF.WindsMPH, LF.PressureMB, BaseDam2014, BaseDamage, NDAM2014, deaths). The windspeed categories were created as a variable using the existing variable LF.WindsMPH based on the Saffir-Simpson Hurricane Wind Scale. MANOVA assumptions of multivariate normality for each group were violated because the p values for each group were less than 0.05, so we rejected the null hypothesis. Thus, assumptions for MANOVA were violated, but the test was continued. Significant differences were found among the five windspeed catefories for at least one of the dependent variables (pillai trace = 1.5287, pseudo F(24, 348) = 8.9694, p < 0.00135).

Univariate ANOVAS for each dependent variable were conducted as follow-up tests to the MANOVA, and the Bonferroni method was used to control for Type I error rates for multiple comparisons. The univariate ANOVAS for LF.WindsMPH, LF.PressureMB, and NDAM2014 were significant, F(4, 89) = 639.93  and p < 0.00135, F(4, 89) = 61.878 and p =< 0.00135, and F(4, 89) = 9.457 and p =< 0.00135, respectively, while the univariate ANOVAS for the other variables were not significant. This means that LF.WindsMPH, LF.PressureMB, and NDAM2014 show a significant mean difference across groups.

One MANOVA, six ANOVAS, and 30 post-hoc t tests were performed (a total of 37 tests), so the probability of making at least one type I error is 0.8501097, and the adjusted significance level is 0.001351351 based on the Bonferroni correction. Post hoc analysis was performed to determine which windspeed category differed in LF.WindsMPH, LF.PressureMB, and NDAM2014. In terms of LF.WindsMPH, categories One and Five, Two and Five, Three and Five, Four and Five, One and Four, Two and Four, Three and Four, One and Two, One and Three, and Two and Three differed significantly from each other after adjusting for multiple comparisons (bonferroni α = .05/37=0.001351351). In terms of LF.PressureMB, categories One and Five, Two and Five, Three and Five, One and Four, Two and Four, One and Two, One and Three, and Two and Three differed significantly from each other after adjusting for multiple comparisons (bonferroni α = .05/37=0.001351351). In terms of NDAM2014, categories One and Five, Two and Five, Three and Five, One and Four, and Two and Four differ significantly from each other after adjusting for multiple comparisons (bonferroni α = .05/37=0.001351351).

#Randomization Test for Mean Differences
```{r}
  #Visualizing Distribution of Response Variable per Name: appears non-normal
ggplot(hurricane,aes(LF.WindsMPH,fill=mf))+geom_histogram(bins=6.5)+
  facet_wrap(~mf,ncol=2)+theme(legend.position="none")

  #True Mean Difference
hurricane%>%group_by(mf)%>% summarize(means=mean(LF.WindsMPH))%>%summarize(`mean_diff`=diff(means))

  #Randomization Test: Monte Carlo Sample
set.seed(1234)
rand_h<-vector() 
for(i in 1:5000){
new_h<-data.frame(winds=sample(hurricane$LF.WindsMPH), name=hurricane$mf) 
rand_h[i]<-mean(new_h[new_h$name=="f",]$winds)-   
              mean(new_h[new_h$name=="m",]$winds)}

  #Visualizing Null Distribution and Test Statistic
{hist(rand_h,main="",ylab=""); abline(v = c(-3.203125	, 3.203125),col="red")}

  #P-value for Permutation Test
mean(rand_h>3.203125 | rand_h< -3.203125)
```
A randomization test was performed to see whether there was a difference in mean windspeeds (MPH) of hurricanes between male-named (m=30) and female-named (n=64) hurricanes. Assumptions for the independent t test were violated. The null hypothesis was that there were no associations of mean windspeeds (MPH) between male-named and female-named hurricanes (the true difference in means was equal to 0). The alternative hypothesis was that there were associations of mean windspeeds (MPH) between male-named and female-named hurricanes (the true difference in means was not equal to 0). The actual mean difference between groups was calculated to be 3.203125, where female-named hurricanes had a mean windspeed 3.203125 higher than male-named hurricanes. After performing the randomization test by taking 5000 random permutations, the p-value of 0.5194 corresponded to the probability of observing a mean difference as extreme as the one under the randomization distribution. This p-value represented the proportion of all 5000 permutation mean differences that were greater than 3.203125 and less than -3.203125. With a p-value of 0.5194, we failed to reject the H0, signifying that there was no significant association between mean windspeeds of male-named and female-named hurricanes. A plot was created to visualize the null distribution and the test statistic, showing where the true mean difference lay with respect to the windspeeds plotted. The plot showed that the test statistic did not fall far from the curve of the null distribution, thus relating to the high p-value. 

#Linear Regression Model
```{r}
  #Model
hurricane_lm <- hurricane
hurricane_lm$Winds_c <- hurricane_lm$LF.WindsMPH - mean(hurricane_lm$LF.WindsMPH)
hurricane_lm$Pressure_c <- hurricane_lm$LF.PressureMB - mean(hurricane_lm$LF.PressureMB)
fit_lin <- lm(deaths ~ Winds_c * Pressure_c, data=hurricane_lm)
summary(fit_lin)

  #Plot
library(interactions)
interact_plot(fit_lin, pred = Pressure_c, modx = Winds_c, plot.points = TRUE)

  #Assumptions:
resids_lin <- fit_lin$residuals
fitvals_lin <- fit_lin$fitted.values

#Linearity, Homoskedasticity
ggplot()+geom_point(aes(fitvals_lin,resids_lin))+geom_hline(yintercept=0, color='red') #Looks decently linear, reject Ho of homoskedasticity

#Homoskedasticity Only
library(sandwich)
library(lmtest) 
bptest(fit_lin) #reject Ho

#Normality
ks.test(resids_lin, "pnorm", mean=0, sd(resids_lin)) #reject Ho
shapiro.test(resids_lin) #reject Ho

  #Robust SEs
coeftest(fit_lin, vcov = vcovHC(fit_lin))

#R^2 is the proportion of variation in the response variable explained by the overall model; Multiple R-squared:  0.1695,	Adjusted R-squared:  0.1418 
```
Regarding the coefficient estimates, the intercept shows that the number of deaths is estimated to be 16.49871 at average wind speed (Winds_c) and at average atmospheric pressure (Pressure_c). The coefficient estimate of Winds_c= -4.26630 means that, at average atmospheric pressure, the number of deaths decreases by 4.26630 for every 1 unit increase in wind speed, on average. There is a negative effect of windspeed on deaths when atmospheric pressure is average. The coefficient estimate of Pressure_c= -6.66412 means that, at average wind speed, the number of deaths decreases by 6.66412 for every 1 unit increase in atmospheric pressure, on average. There is a negative effect of atmospheric pressure on deaths at average wind speed. The coefficient estimate of Winds_c:Pressure_c= -0.07450 means that the slope for atmospheric pressure on number of deaths is 0.07450 lower for every 1 unit increase in wind speed. Thus, for every 1 unit increase in wind speed, the predicted atmospheric pressure decreases by 0.07450.

The plot of the regression shows predicted values of Pressure_c by deaths when Winds_c is at the mean, mean+sd, and mean-sd values. The plot shows that for wind speeds at the mean value, less than the mean value, and above the mean value, an increase in atmospheric pressure has a decrease in the number of deaths for all windspeed groups. However, a wind speed above the mean with a high atmospheric pressure has the least number of deaths. For wind speeds below the average, the slope is less steep, representing a smaller negative relationship between wind speeds below the average and the number of deaths with increasing pressure. For wind speeds above the average, the slope is more steep (a larger negative value), representing a higher negative relationship between wind speeds above average and the number of deaths with increasing pressure.

Checking assumptions, the plot of fitted values versus residual values shows a relatively linear relationship between the predictor and response, but has a few drastic outliers which leads to a "fanning out" of the points, perhaps signaling heteroskedsaticity. Using the Breusch-Pagan test to formally assess homoskedasticity, the null hypothesis of homoskedasticity is rejected with a p-value of 0.01265. Doing the Kolmogorov-Smirnov test and the Shapiro-Wilk test to check for normality, the null hypothesis of the true distribution being normal is rejected with a p-value of less than 0.05 for both. Thus, many assumptions such as normally distributed residuals and equal variance of residuals along the regression line are violated. 

Recomputing regression results with robust standard errors, the effects of Winds_c, Pressure_c, and Winds_c:Pressure_c are no longer significant. These effects were significant before recomputing the regression results. After redoing the regression using heteroskedasticity robust standard errors, the magnitude of the t-value for the slope of Winds_c became smaller, going from 2.402 to 1.0218, and the slope was no longer significant. The magnitude of the t-value for the slope of Pressure_c became smaller, going from 3.604 to 1.1659, and the slope was no longer significant. The magnitude of the t-value for the slope of Winds_c:Pressure_c became smaller, going from 2.091 to 1.3114, and the slope was no longer significant. R^2 is the proportion of variation in the response variable explained by the overall model, and the multiple R-squared value is 0.1695 while the adjusted R-squared is 0.1418. Based on the adjusted R^2 value, 14.18% of the variation in deaths can be attributed to this model with the variables Winds_c and Pressure_c. 

#Bootstrapped SE
```{r}
set.seed(1234)
samp_distn1<-replicate(5000, {
  boot_dat1 <- sample_frac(hurricane_lm, replace=T) 
  fit_lin <- lm(deaths ~ Winds_c * Pressure_c, data=boot_dat1)
  coef(fit_lin)
}) 

samp_distn1 %>% t %>% as.data.frame %>% summarize_all(sd)

samp_distn1 %>% t %>% as.data.frame %>% pivot_longer(1:3) %>% group_by(name) %>%
 summarize(lower=quantile(value,.025), upper=quantile(value,.975)) 
```
After computing the bootstrapped standard errors, the SE for Winds_c (3.755543) was higher than the original SE (1.77607) and slightly lower than the robust SE (4.175406), the SE for Pressure_c (5.087792) was higher than the original SE (1.84925) and slightly lower than the robust SE (5.715888), and the SE for Winds_c:Pressure_c (0.06906535) was higher than both the original SE (0.03562) and robust SE (0.056812). The bootstrapped SE for Winds_c and Pressure_c were slightly lower than the robust SE for the same variables, meaning the bootstrapped Winds_c and Pressure_c p-values would be slightly lower than the robust Winds_c's and Pressure_c's as well. Since the robust Winds_c and Pressure_c p-values were much larger than 0.05, the bootstrapped SE's p-values would still likely be greater than 0.05, and the effects would still be insignificant. The bootstrapped SE for Winds_c:Pressure_c was higher than the robust SE for the same interaction, meaning the bootstrapped Winds_c:Pressure_c p-value would be higher than the robust Winds_c:Pressure_c's. Since the robust Winds_c:Pressure_c p-value was much larger than 0.05, the bootstrapped interaction's p-value would still be greater than 0.05, making this effect insignificant. Since these bootstrapped SEs were more similar in value to the robust SEs than the original SEs, the p-values would likely have remained higher than 0.05 and the effects would still be insignificant.

#Logistic Regression Model Pt.1
```{r}
  #Model
hurricane1 <- hurricane
fit_log <- glm(mf ~ Year + BaseDam2014 + deaths, family="binomial", data=hurricane1)
coeftest(fit_log)

exp(coef(fit_log))

  #Confusion Matrix
hurricane1$prob <- predict(fit_log, type="response")
table(prediction=as.numeric(hurricane1$prob>.5), truth=hurricane$mf)%>%addmargins

(56+7)/94 #Accuracy
7/30 #Sensitivity
56/64 #Specificity
7/15 #Precision

  #Density Plot
hurricane1$logit<-predict(fit_log,type="link")
hurricane1%>%ggplot()+geom_density(aes(logit,color=mf,fill=mf), alpha=.4)+
  theme(legend.position=c(.85,.85))+geom_vline(xintercept=0)+xlab("logit (log-odds)")+
  geom_rug(aes(logit,color=mf))

  #ROC curve
library(plotROC)
ROCplot_h <- ggplot(hurricane1)+geom_roc(aes(d=mf, m=prob), n.cuts=0)+geom_segment(aes(x=0, xend=1, y=0, yend=1), lty=2)
ROCplot_h
calc_auc(ROCplot_h)
```
Looking at the coefficient estimates of the logistic regression model, the intercept shows that the odds of the hurricane being a male name for Year=0, BaseDam2014=0, and deaths=0 is 9.810789e-32. Controlling for BaseDam2014 and deaths, for every one additional increase in Year, the odds of being a male-named hurricane increases by a factor of 1.036328. Controlling for Year and deaths, for every one additional increase in BaseDam2014, the odds of being a male-named hurricane increases by a factor of 1.000037. Controlling for Year and BaseDam2014, for every one additional death, the odds of being a male-named hurricane increases by a factor of 9.826603e-01. The confusion matrix shows an accuracy value of 0.6702128, a sensitivity value of 0.2333333, a specificity value of 0.875, and a precision value of 0.4666667. The AUC value is 0.7177083, which is a "fair" value. The density plot of the log-odds visualizes the difficulty in predicting whether the hurricane is male or female-named based on having a probability of above 0.5 or below 0.5. The ROC curve visualizes the trade-off between sensitivity and specificity. If we were predicting perfectly, TPR would be 1 while FPR would be 0 for any cutoff except 100%. With this hurricane data, the ROC shows that we are not predicting perfectly, and the AUC value (or the area under the curve) of 0.7177083 demonstrates that the classifer performance is "fair." 

#Logistic Regression Model Pt.2
```{r}
  #Model
hurricane2 <- hurricane%>%mutate(y=ifelse(mf=="m",1,0))
hurricane2 <- hurricane2 %>%select(-Name, -AffectedStates, -mf)
fit_all <- glm(y~(.)^2, data=hurricane2, family="binomial")
coef(fit_all) %>% head(15)

  #In-sample Classification Diagnostics
class_diag <- function(probs,truth){
  #CONFUSION MATRIX: CALCULATE ACCURACY, TPR, TNR, PPV
  tab<-table(factor(probs>.5,levels=c("FALSE","TRUE")),truth)
  acc=sum(diag(tab))/sum(tab)
  sens=tab[2,2]/colSums(tab)[2]
  spec=tab[1,1]/colSums(tab)[1]
  ppv=tab[2,2]/rowSums(tab)[2]
  f1=2*(sens*ppv)/(sens+ppv)

  if(is.numeric(truth)==FALSE & is.logical(truth)==FALSE) truth<-as.numeric(truth)-1
  
  #CALCULATE EXACT AUC
  ord<-order(probs, decreasing=TRUE)
  probs <- probs[ord]; truth <- truth[ord]
  
  TPR=cumsum(truth)/max(1,sum(truth)) 
  FPR=cumsum(!truth)/max(1,sum(!truth))
  
  dup<-c(probs[-1]>=probs[-length(probs)], FALSE)
  TPR<-c(0,TPR[!dup],1); FPR<-c(0,FPR[!dup],1)
  n <- length(TPR)
  auc<- sum( ((TPR[-1]+TPR[-n])/2) * (FPR[-1]-FPR[-n]) )

  data.frame(acc,sens,spec,ppv,f1,auc)
}

prob_all <- predict(fit_all, type="response")
class_diag(prob_all, hurricane2$y)

  #10-fold CV
set.seed(1234)
k=10
data<-hurricane2[sample(nrow(hurricane2)),]
folds<-cut(seq(1:nrow(hurricane2)),breaks=k,labels=F)
diags<-NULL
for(i in 1:k){
  train<-data[folds!=i,] 
  test<-data[folds==i,]
  truth<-test$y
  fit <- glm(y~(.)^2, data=train, family="binomial")
  probs<-predict(fit,newdata = test,type="response")
  diags<-rbind(diags,class_diag(probs,truth))
}

summarize_all(diags,mean) 

  #LASSO
library(glmnet)
y<-as.matrix(hurricane2$y)
x<-model.matrix(y~.,data=hurricane2)[,-1]
head(x)

cv <- cv.glmnet(x,y, family="binomial")
x<-scale(x)

{plot(cv$glmnet.fit, "lambda", label=TRUE); abline(v = log(cv$lambda.1se)); abline(v = log(cv$lambda.min),lty=2)} 

cv<-cv.glmnet(x,y,family="binomial")
lasso<-glmnet(x,y,family="binomial",lambda=cv$lambda.1se)
coef(lasso) #retain Year

  #10-fold on LASSO variables
set.seed(1234)
k=10
data <- hurricane2 %>% sample_frac 
folds <- ntile(1:nrow(data),n=10) 

diags<-NULL
for(i in 1:k){
  train <- data[folds!=i,]
  test <- data[folds==i,]
  truth <- test$y 
  fit <- glm(y~Year, data=train, family="binomial")
  probs <- predict(fit, newdata=test, type="response")
  
  diags<-rbind(diags,class_diag(probs,truth))
}

diags%>%summarize_all(mean)
```
The in-sample classification diagnostics show an accuracy of 0.8723404, sensitivity of 0.8, specificity of 0.90625, precision of 0.8, and an AUC of 0.853125 which is a "good"" value but is on a flexible model. After performing the 10-fold CV with the same model, the accuracy had a value of 0.4055556, sensitivity had a value of 0.415, specificity had a value of 0.4565476, precision had a value of 0.302619, and AUC had a value of 0.4441071. This AUC value demonstrates that the classifer performance is very bad, and this significant drop in AUC from the original is a sign of overfitting because this model did much more poorly than the original in-sample metrics. After performing LASSO, only the variable "Year" was retained meaning that this was the only important predictor. Performing 10-fold CV using only the variable "Year," the AUC value was 0.635377 which is a "poor" value. However, it is higher than the AUC value from the 10-fold CV with all variables. It is still significantly lower than the AUC value from the in-sample metrics. LASSO did reduce overfitting, and the less complex model made better predictions than the original 10-fold CV with all the variables; however, it still had a lower AUC value than the in-sample classification diagnostics.
